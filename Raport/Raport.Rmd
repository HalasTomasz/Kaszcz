---
title: "Komputerowa analiza szeregów czasowych - raport 1"
author: "Szymon Malec, Tomasz Hałas"
output:
  pdf_document: 
    extra_dependencies: ["polski", "mathtools", "amsthm", "amssymb", "icomma", "upgreek", "xfrac", "scrextend", "float", "tabularx", "hyperref", "caption", "enumitem"]
fontsize: 12pt
---

\renewcommand{\figurename}{Wykres}
\renewcommand{\tablename}{Tablica}
\raggedbottom

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval = TRUE, fig.pos = "H", dev.args=list(encoding="CP1257.enc"))
```

```{r}
library(ggplot2)
library(dplyr)
library(zeallot)
library(tidyr)
library(knitr)
library(reshape2)
library(cowplot)
library(latex2exp)

regression <- function(X, Y){
    r <- cor(X, Y, use="pairwise.complete.obs")
    Sx <- sd(X)
    Sy <- sd(Y)
    a <- r * Sy / Sx
    b <- mean(Y) - a * mean(X)
    return(c(a, b))
}

data <- read.csv('data/data.csv')
data2015 <- data %>% filter(year == 2015)
data_filtered <- data2015 %>% filter(!is.na(schooling) & !is.na(life_expectancy))

X <- data_filtered$schooling
Y <- data_filtered$life_expectancy
c(a, b) %<-% regression(X, Y)
E <- Y - a*X - b
```




\section{Wstęp}
<!-- Akapit - 6 spacji -->
|      Celem raportu jest zbadanie liniowej korelacji pomiędzy edukacją, a długością życia. Wykorzystane do tego zostaną dane dostępne pod [$\color{blue}{\text{linkiem}}$](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who?fbclid=IwAR2HtwUPyioM4tHmuae7B2owTUB8q3XlmpP12LbTM9NYDsi4qtaWGOYoNDE). Przeprowadzimy dokładną analizę zależności między dwoma zmiennymi oraz zastosujemy odpowiednie metody, by dopasować prostą regresji do danych, po czym zweryfikujemy czy dopasowanie można uznać prawidłowe. Otrzymane wyniki wykorzystane zostaną do przeprowadzenia predykcji długości życia w zależności od czasu edukacji.





\section{Opis danych}
|      Dane, z których będziemy korzystać, opisują wiele cech 183 państw w latach 2000-2015. My jednak skorzystamy wyłącznie z oczekiwanej długości życia i średniego czasu edukacji w 2015 roku, czyli najbardziej aktualnych danych. Dla tego roku zawarta jest pełna informacja o 173 krajach.

```{r scatter, fig.cap="\\label{fig:scatter} Wykres punktowy badanych danych.", fig.align="center", fig.width = 4, fig.height = 3}
ggplot() +
  geom_point(aes(X, Y), alpha=0.6, size=1) +
  xlab("Czas nauki") +
  ylab("Długość życia")
```

Na powyższym wykresie widzimy wyraźną zależność liniową pomiędzy dwoma zmiennymi. W dalszej części poddamy ją głębszej analizie.





\section{Statystyki}






\section{Regresja}
|      Aby dopasować do danych prostą regresji, skorzystamy z metody najmniejszych kwadratów. Oznaczmy licznę danych (państw) jako $n = 173$. Przyjmijmy model
$$ Y_i = \beta_1 x_i + \beta_0 + \epsilon_i, \ \ \ i = 1, 2, \dots, n $$
gdzie $x_i$ to dane dotyczące czasu nauczania, a $\epsilon_i$ są niezależnymi zmiennymi losowymi ze średnią równą 0 i skończoną wariancją. Oznaczmy dane z czasem życia jako $y_i$ - będziemy traktować je jako realizacje zmiennych losowych $Y_i$. Wspomniana metoda polega na znalezieniu takich współczynników $\beta_1, \beta_0$, dla których funkcja
$$ S(\beta_1, \beta_0) = \sum_{i = 1}^n (y_i - \beta_1 x_i - \beta_0)^2 $$
przyjmuje wartość najmniejszą. Rozwiązaniem jest para estymatorów
$$
    \begin{cases}
      \hat{\beta_1} = R \dfrac{S_y}{S_x} = \dfrac{\sum\limits_{i = 1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i = 1}^n(x_i - \bar{x})^2}\\
      \hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}
    \end{cases}
$$
gdzie $R$ jest współczynnikiem korelacji Pearsona, a $S_x, S_y$ są próbkowymi odchyleniami standardowymi. Można pokazać, że estymatory te są nieobciążone. Po podstawieniu danych otrzymujemy
$$
    \begin{cases}
      \hat{\beta_1} \approx 2.23 \\
      \hat{\beta_0} \approx 42.9
    \end{cases}.
$$
Wyestymowane wartości $Y_i$ będą miały postać
$$ \hat{y_i} = \hat{\beta_1} x_i + \hat{\beta_0}. $$

```{r regresja, fig.cap="\\label{fig:regresja} Wykres punktowy wraz z prostą regresji wyznaczona dla danych.", fig.align="center", fig.width = 6, fig.height = 3}
ggplot() + 
  geom_point(aes(X, Y, col='a'), size=0.5) + 
  geom_line(aes(X, a*X + b, col='b'), linewidth=0.5) + 
  scale_color_manual(labels=c("Dane", "Prosta regresji"), values=c('#008cff', "#da350b")) +
  labs(col="") +
  xlab("Czas nauki") +
  ylab("Długość życia")
```

Jak możemy zauważyć wyznaczona prosta pokrywa się z danymi. Wspołczynnik determinancji, czyli $R^2$ wyniósł 0.67, a więc jest to dość zadowalające dopasowanie.





\section{Analiza residuów}

|      Aby sprawdzić, czy dane spełniają założenia modelu tj.
\begin{enumerate}
  \item $ \mathrm{E} \epsilon_i = 0 $,
  \item $ \mathrm{Var}(\epsilon_i) < \infty $,
  \item $ \epsilon_i $ są niezależne,
\end{enumerate}
przeprowadzimy analizę residuów (błędów)
$$e_i = y_i - \hat{y_i},$$
czyli realizacji zmiennych $\epsilon_i$. Ponieważ estymatory $\hat{\beta_1}, \hat{\beta_0}$ zostały wyznaczone metodą najmniejszych kwadratów, to pierwsze założenie na pewno jest spełnione. Dodatkowo, jeśli spojrzymy na wykres \ref{fig:res_scatter}, residua wydają się być rozłożone losowo wokół zera.

```{r res_scatter, fig.cap="\\label{fig:res_scatter} Wykres punktowy residuów.", fig.align="center", fig.width = 4, fig.height = 3}
df <- data.frame(X=X, Y=Y, E=E)
df <- df %>% arrange(X)
ggplot() + 
  geom_point(aes(1:length(E), df$E), size=0.5, col="#1b65f9", alpha=0.7) + 
  geom_line(aes(1:length(E), 0), linewidth=0.5) +
  labs(col="") +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  ylab(TeX("$e_i$"))
```

|      W przypadku założenia drugiego obliczymy wariancje częściowe postaci
$$ S^2_k = \frac{1}{k - 1} \sum_{i=1}^k e_i^2, \ \ \ k = 2, \dots, n. $$

```{r res_wariancja, fig.cap="\\label{fig:res_wariancja} Porównanie wariancji częściowej $S^2_k$ residuów dla $k = 2,... , n$ z wariancją z całej próby, czyli $S^2_n$.", fig.align="center", fig.width = 6, fig.height = 3}
n <- length(E)
varE <- c()
for (i in 2:n) {
    varE <- append(varE, var(E[1:i]))
}
ggplot() + 
  geom_line(aes(2:n, varE, col='a')) + 
  geom_line(aes(2:n, var(E), col='b')) +
  scale_color_manual(labels=c(TeX("$S^2_k$"), TeX("$S^2_n$")), values=c('#00b3ff', "#d36f12")) +
  labs(col="") +
  xlab(TeX("$k$")) +
  ylab("Wariancja")
```

Na powyższym wykresie zobaczyć możemy, że wariancja częściowa wraz ze zwiększającym się $k$ szybko zbiega do wartości wariancji z całej próbki i oscyluje wokół niej. Na tej podstawie możemy zakładać, że $\mathrm{Var}(\epsilon_i)$ jest skończona i stała.

|      Aby zweryfikować, czy spełnione jest założenie trzecie, skorzystamy z funkcji empirycznej autokorelacji o postaci
$$ \hat{\rho}(h) = \dfrac{\hat{\gamma}(h)}{\hat{\gamma}(0)}, $$
gdzie
$$ \hat{\gamma}(h) = \frac{1}{n} \sum_{i=1}^{n - |h|} (e_{i + |h|} - \bar{e}) (e_i - \bar{e}) $$
jest estymatorem funkcji autokowariancji.

```{r res_korelacja, fig.cap="\\label{fig:res_korelacja} Wykres punktowy empirycznej autokorelacji w zależności od przesunięcia $h$.", fig.align="center", fig.width = 4, fig.height = 3}
n <- length(E)
hs <- 0:50
Xt <- rnorm(n, 0, sd(E))

g_ <- function(X, h) {
    return ( mean( (X[(1+h):n] - mean(X)) * (X[1:(n-h)] - mean(X)) ) )
}

g_s <- c()
g_s2 <- c()
for (h in hs) {
    g_s <- append(g_s, g_(E, h))
    g_s2 <- append(g_s2, g_(Xt, h))
}

r_s = g_s / g_(E, 0)
r_s2 = g_s2 / g_(Xt, 0)

ggplot() + 
  geom_point(aes(hs, r_s), size=1) +
  labs(col="") +
  xlab(TeX("$h$")) +
  ylab(TeX('$\\hat{\\rho}(h)$'))
```

Na wykresie \ref{fig:res_korelacja} widać, że dla $h \neq 0$ funkcja $\hat{\rho}(h)$ oscyluje wokół zera, zatem wnioskujemy, że residua są od siebie niezależne.

|      Teraz postaramy się znaleźć rozkład błędów $e_i$. Zaczniemy od spojrzenia na ich histogram (wykres \ref{fig:res_histogram}).

```{r res_histogram, fig.cap="\\label{fig:res_histogram} Histogram residuów.", fig.align="center", fig.width = 4, fig.height = 3}
ggplot() + 
  geom_histogram(aes(x=E, y=..density..), bins=13) +
  xlab(TeX("$e_i$")) +
  ylab(TeX("Gęstość"))
```

Kształt histogramu przypomina nieco rozkład normalny. Załóżmy, że w rzeczywistości tak jest. Wtedy nieobciążonym estymatorem wariancji jest
$$ S^2 = \frac{1}{n - 2} \sum_{i=1}^n e_i^2 \approx 20.93 \ . $$

```{r res_gestosc, fig.cap="\\label{fig:res_gestosc} Porównanie histogramu z danych z gęstością teoretyczną rozkładu $\\mathcal{N}(0, S^2)$.", fig.align="center", fig.width = 6, fig.height = 3}
S <- sum(E^2) / (n - 2)
xs <- seq(min(E), max(E) + 3, 0.01)
ggplot() + 
  geom_histogram(aes(x=E, y=..density.., fill='a'), bins=13) + 
  geom_line(aes(xs, dnorm(xs, 0, sqrt(S)), col='b'), linewidth=1) + 
  scale_color_manual(labels=c("gęstość teoret."), values=c('#f13b3b')) +
  scale_fill_manual(labels=c("histogram"), values=c('#586974')) +
  labs(col="", fill="") +
  xlab(TeX("$e_i$")) +
  ylab("Gęstość")
```

```{r res_dystrybuanta, fig.cap="\\label{fig:res_dystrybuanta} Porównanie dystrybuanty empirycznej z danych z dystrybuantą teoretyczną rozkładu $N(0, S^2)$.", fig.align="center", fig.width = 6, fig.height = 3}
F <- ecdf(E)
xs <- seq(min(E), max(E), 0.01)
ggplot() + 
  geom_line(aes(x=xs, y=F(xs), col='a'), linewidth=0.5) + 
  geom_line(aes(x=xs, y=pnorm(xs, 0, sqrt(S)), col='b'), linewidth=0.5) +
  scale_color_manual(labels=c("Dystrybuanta empir.", "Dystrybuanta teoret."), values=c('#00b3ff', "#d36f12")) +
  labs(col="") +
  xlab(TeX("$e_i$")) +
  ylab("Dystrybuanta")
```

Na wykresie \ref{fig:res_gestosc} zauważamy, że histogram w miarę pokrywa się z gęstoością teoretyczną. Nie spodziewamy się tutaj bardzo dokładnego pokrywania ze względu na niewielką liczbę danych. Z kolei na wykresie \ref{fig:res_dystrybuanta} widzimy, że dystrybuanta empiryczna wyraźnie nakłada się z dystrybuantą teoretyczną. Aby umocnić nasze przekonania co do normalności residuów, przeprowadzimy test Kołmogorowa-Smirnowa. Przedstawmy hipotezy:

\begin{itemize}
\item $\mathcal{H}_0$: wartości residuów są z rozkładu $\mathcal{N}(0, S^2)$
\item $\mathcal{H}_1$: wartości residuów nie są z rozkładu $\mathcal{N}(0, S^2)$
\end{itemize}

Otrzymana p-wartość testu wynosi 0,2532. Ponieważ otrzymany wynik jest wystarczająco duży, to nie mamy podstaw do odrzucenia hipotezy zerowej i możemy przyjąć, że dane pochodzą z rozkładu $\mathcal{N}(0, S^2)$.





\section{Przedziały ufności}





\section{Predykcja}

