---
title: "Komputerowa analiza szeregów czasowych - raport 1"
author: "Szymon Malec, Tomasz Hałas"
output:
  pdf_document: 
    extra_dependencies: ["polski", "mathtools", "amsthm", "amssymb", "icomma", "upgreek", "xfrac", "scrextend", "float", "tabularx", "hyperref", "caption", "enumitem"]
fontsize: 12pt
---

\renewcommand{\figurename}{Wykres}
\renewcommand{\tablename}{Tablica}
\raggedbottom

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval = TRUE, fig.pos = "H")
```

```{r}
library(ggplot2)
library(dplyr)
library(zeallot)
library(tidyr)
library(knitr)
library(reshape2)
library(cowplot)

regression <- function(X, Y){
    r <- cor(X, Y, use="pairwise.complete.obs")
    Sx <- sd(X)
    Sy <- sd(Y)
    a <- r * Sy / Sx
    b <- mean(Y) - a * mean(X)
    return(c(a, b))
}

data <- read.csv('data/data.csv')
data2015 <- data %>% filter(year == 2015)
data_filtered <- data2015 %>% filter(!is.na(schooling) & !is.na(life_expectancy))

X <- data_filtered$schooling
Y <- data_filtered$life_expectancy
c(a, b) %<-% regression(X, Y)
E <- Y - a*X - b
```




\section{Wstęp}
<!-- Akapit - 6 spacji -->
|      Celem raportu jest ...





\section{Opis danych}

```{r scatter, fig.cap="\\label{fig:scatter} Wykres punktowy badanych danych.", fig.align="center", fig.width = 4, fig.height = 3}
ggplot() + 
  geom_point(aes(X, Y), alpha=0.6, size=2.2) + 
  xlab("Czas nauki") +
  ylab("Długość życia")
```





\section{Statystyki}






\section{Regresja}
|      Jako pierwszy przeanalizujemy wpływ czasu edukacji na oczekiwaną długość życia. W tym przypadku rozważymy dane wyłącznie z 2015 roku, czyli te najbardziej aktualne. Na wykresie \ref{fig:regresja} można zauważyć liniową zależność danych, zatem do zbadania korelacji możemy użyć współczynnika Pearsona. Przyjmuje on wartość $R \approx 0,752$, więc jest to dość mocna korelacja. Dopasujemy teraz do danych prostą regresji korzystając z metody najmniejszych kwadratów. Przyjmijmy model
$$ Y_i = ax_i + b + \epsilon_i, $$
gdzie $x_i$ to dane dotyczące czasu nauczania, a $\epsilon_i$ są i.i.d. ze średnią równą 0 i skończoną wariancją. Oznaczmy dane z czasem życia jako $y_i$. Wspomniana metoda polega na znalezieniu takich współczynników $a, b$ dla których funkcja
$$S(a,b) = \sum_{i = 1}^n (y_i - ax_i - b)^2$$
przyjmuje wartość najmniejszą. Rozwiązaniem jest para estymatorów
$$
    \begin{cases}
      \hat{a} = R\frac{S_y}{S_x} = \frac{\sum_{i = 1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^n(x_i - \bar{x})^2}\\
      \hat{b} = \bar{y} - a\bar{x}
    \end{cases}
$$
gdzie $R$ jest współczynnikiem korelacji Pearsona, a $S_x, S_y$ są próbkowymi odchyleniami standardowymi.

```{r regresja, fig.cap="\\label{fig:regresja} Wykres punktowy wraz z prostą regresji wyznaczona dla danych.", fig.align="center", fig.width = 4, fig.height = 3}
ggplot() + 
  geom_point(aes(X, Y, col='a')) + 
  geom_line(aes(X, a*X + b, col='b'), linewidth=1) + 
  scale_color_manual(labels=c("Dane", "Prosta regresji"), values=c('#008cff', "#da350b")) +
  labs(col="") +
  xlab("Czas nauki") +
  ylab("Długość życia")
```





\section{Analiza residuów}

Kolejnym punktem będzie analiza residuów (błędów)
$$e_i = y_i - \hat{y}_i\;,$$
gdzie $\hat{y}_i = \hat{a}x_i + \hat{b}$. W celu zbadania rozkładu residuów, spójrzmy na ich histogram

```{r res_scatter, fig.cap="\\label{fig:res_scatter} Histogram residuów.", fig.align="center", fig.width = 4, fig.height = 3}
df <- data.frame(X=X, Y=Y, E=E)
df <- df %>% arrange(X)
ggplot() + 
  geom_point(aes(1:length(E), df$E)) + 
  geom_line(aes(1:length(E), 0), linewidth=1) +
  labs(col="") +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank()) +
  ylab("Wartość błędu")
```

```{r res_histogram, fig.cap="\\label{fig:res_histogram} Histogram residuów.", fig.align="center", fig.width = 4, fig.height = 3}
ggplot() + geom_histogram(aes(x=E, y=..density..), bins=13)
```

```{r res_gestosc, fig.cap="\\label{fig:res_gestosc} Porównanie histogramu z danych z gęstością teoretyczną rozkładu $N(0, s)$.", fig.align="center", fig.width = 6, fig.height = 3}
xs <- seq(min(E), max(E) + 3, 0.01)
ggplot() + 
  geom_histogram(aes(x=E, y=..density.., fill='a'), bins=13) + 
  geom_line(aes(xs, dnorm(xs, 0, sd(E)), col='b'), linewidth=1.5) + 
  scale_color_manual(labels=c("gęstość teoret."), values=c('#f13b3b')) +
  scale_fill_manual(labels=c("histogram"), values=c('#586974')) +
  labs(col="", fill="") +
  xlab("Wartość błędu") +
  ylab("Gęstość")
```

```{r res_dystrybuanta, fig.cap="\\label{fig:res_dystrybuanta} Porównanie dystrybuanty empirycznej z danych z dystrybuantą teoretyczną rozkładu $N(0, s)$.", fig.align="center", fig.width = 6, fig.height = 3}
F <- ecdf(E)
xs <- seq(min(E), max(E), 0.01)
ggplot() + 
  geom_line(aes(x=xs, y=F(xs), col='a'), linewidth=1) + 
  geom_line(aes(x=xs, y=pnorm(xs, 0, sd(E)), col='b'), linewidth=1) +
  scale_color_manual(labels=c("Dystrybuanta empir.", "Dystrybuanta teoret."), values=c('#00b3ff', "#d36f12")) +
  labs(col="") +
  xlab("Wartość błędu") +
  ylab("Dystrybuanta")
```

```{r res_korelacja, fig.cap="\\label{fig:res_korelacja} Wykres punktowy empirycznej autokorelacji w zależności od przesunięcia $h$.", fig.align="center", fig.width = 4, fig.height = 3}
n <- length(E)
hs <- 0:70
Xt <- rnorm(n, 0, sd(E))

g_ <- function(X, h) {
    return ( mean( (X[(1+h):n] - mean(X)) * (X[1:(n-h)] - mean(X)) ) )
}

g_s <- c()
g_s2 <- c()
for (h in hs) {
    g_s <- append(g_s, g_(E, h))
    g_s2 <- append(g_s2, g_(Xt, h))
}

r_s = g_s / g_(E, 0)
r_s2 = g_s2 / g_(Xt, 0)

ggplot() + 
  geom_point(aes(hs, r_s), size=2.2) +
  #scale_color_manual(labels=c("", ""), values=c('#00b3ff', "#d36f12")) +
  labs(col="") +
  xlab("h") +
  ylab("autokorelacja")
```

Kształt histogramu jest zbliżony do krzywej gaussowskiej. Średnia wartość residuów wynosi $\mu_e = 0$, a ich wariancja $\sigma_e^2 = 20,81$. Posłużymy się testem Kołmogorowa-Smirnova w celu zbadania normalności rozkładu błędów. Przedstawmy hipotezy:

\begin{itemize}
\item $\mathcal{H}_0$: wartości residuów są z rozkładu normalnego $\mathcal{N}(0, 20.81)$
\item $\mathcal{H}_1$: wartości residuów nie są z rozkładu normalnego $\mathcal{N}(0, 20.81)$
\end{itemize}

Wyznaczona p-wartość wynosi $p = 0,2532$. Ponieważ otrzymany wynik jest wystarczająco duży, to nie mamy podstaw do odrzucenia hipotezy zerowej i możemy przyjąć, że dane pochodzą z rozkładu normalnego $\mathcal{N}(0, 20.81)$. 

|      Analiza residuów jest niezwykle istotna, kiedy decydujemy się robić predykcję danych. Znając rozkład błędów, możemy wyznaczyć przedziały ufności o danym poziomie istotności dla przewidywanych wyników. Innymi słowy możemy wyznaczyć prawdopodobieństwo z jakim predykowana wartość zmieści się w konkretnym przedziale. Wyniki, które otrzymaliśmy mogą być podstawą do wykonania takiej predykcji, jednak wcześniej należałoby jeszcze sprawdzić, czy residua są od siebie niezależne oraz czy ich wariancja jest stała.





\section{Przedziały ufności}





\section{Predykcja}

